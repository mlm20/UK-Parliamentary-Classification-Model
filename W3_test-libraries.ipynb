{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # The answer\n",
    "\n",
    "def generate_data_2d(n, mean_val, std, r_xy):\n",
    "    '''\n",
    "    This function generates two pairs of correlated but randomly generated data of\n",
    "    length n and returns them in a pandas dataframe where one column is named\n",
    "    'indep' and the other is named 'dep'.\n",
    "    Pearson's correlation coefficient between x and y is r_xy.\n",
    "    \n",
    "    Input:\n",
    "        -n: number of data points for each dataset\n",
    "        -mean_val: list of means [mean_x, mean_y]\n",
    "        -std: list of standard deviations [std_x, std_y]\n",
    "        -r_xy: correlation coefficient between x and y.\n",
    "    \n",
    "    Returns:\n",
    "    Pandas dataframe with columns = ['indep','dep'] that are correlated. \n",
    "    '''\n",
    "    std_d = np.diag(std)\n",
    "    corr = np.asarray([[1, r_xy],[r_xy,1]])\n",
    "\n",
    "\n",
    "    cov = np.dot(np.dot(std_d, corr), std_d)\n",
    "\n",
    "    data = np.random.multivariate_normal(mean_val, cov, size = n)\n",
    "    data = pd.DataFrame(data = data, columns = ['x', 'y'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "means = [50,80]\n",
    "stds = [15,28]  \n",
    "r = 0.9 \n",
    "df1 = generate_data_2d(100, means, stds, r)\n",
    "df2 = generate_data_2d(50, means, stds, r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score,\\\n",
    "precision_score, classification_report\n",
    "\n",
    "np.random.seed(42)\n",
    "y = np.random.choice([0,1], 100, replace = True)\n",
    "x1 = np.random.choice(np.linspace(8,100, 200), 100, replace = True)\n",
    "x2 = np.random.choice(np.linspace(300, 5000, 800),100, replace = True)\n",
    "x3 = np.random.choice(['Hot', 'Cold'], 100, replace = True)\n",
    "x4 = np.random.choice(['Ali', 'Julia', 'Eduardo'], 100, replace = True)\n",
    "\n",
    "rand_df = pd.DataFrame({'y':y, 'x1':x1, 'x2':x2, 'x3':x3, 'x4':x4})\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "\n",
    "\n",
    "y_test = np.random.choice([0,1], 100, replace = True)\n",
    "x1_test = np.random.choice(np.linspace(8,100, 200), 100, replace = True)\n",
    "x2_test = np.random.choice(np.linspace(300, 5000, 800), 100,  replace = True)\n",
    "x3_test = np.random.choice(['Hot', 'Cold'], 100, replace = True)\n",
    "x4_test = np.random.choice(['Ali', 'Julia', 'Eduardo'], 100, replace = True)\n",
    "rand_df_test = pd.DataFrame({'y':y_test, 'x1':x1_test, 'x2':x2_test, 'x3':x3_test,\\\n",
    "                            'x4':x4_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score,\\\n",
    "precision_score, classification_report\n",
    "\n",
    "np.random.seed(42)\n",
    "y = np.random.choice([0,1], 100, replace = True)\n",
    "x1 = np.random.choice(np.linspace(8,100, 200), 100, replace = True)\n",
    "x2 = np.random.choice(np.linspace(300, 5000, 800),100, replace = True)\n",
    "x3 = np.random.choice(['Hot', 'Cold'], 100, replace = True)\n",
    "x4 = np.random.choice(['Ali', 'Julia', 'Eduardo'], 100, replace = True)\n",
    "\n",
    "rand_df = pd.DataFrame({'y':y, 'x1':x1, 'x2':x2, 'x3':x3, 'x4':x4})\n",
    "\n",
    "\n",
    "y_test = np.random.choice([0,1], 100, replace = True)\n",
    "x1_test = np.random.choice(np.linspace(8,100, 200), 100, replace = True)\n",
    "x2_test = np.random.choice(np.linspace(300, 5000, 800), 100,  replace = True)\n",
    "x3_test = np.random.choice(['Hot', 'Cold'], 100, replace = True)\n",
    "x4_test = np.random.choice(['Ali', 'Julia', 'Eduardo'], 100, replace = True)\n",
    "rand_df_test = pd.DataFrame({'y':y_test, 'x1':x1_test, 'x2':x2_test, 'x3':x3_test,\\\n",
    "                            'x4':x4_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MaxLM\\OneDrive\\Documents\\University\\DE2\\Data Science\\python-data-science\\W3_test-libraries.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MaxLM/OneDrive/Documents/University/DE2/Data%20Science/python-data-science/W3_test-libraries.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgraphviz\u001b[39;00m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MaxLM/OneDrive/Documents/University/DE2/Data%20Science/python-data-science/W3_test-libraries.ipynb#ch0000004?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_iris\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MaxLM/OneDrive/Documents/University/DE2/Data%20Science/python-data-science/W3_test-libraries.ipynb#ch0000004?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m tree\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import graphviz \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render(\"iris\") \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn import  svm, metrics\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "\n",
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, classification_report(expected, predicted)))\n",
    "print(\"CM:\\n%s\" % confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
    "# output model is the same for precision and recall with ties in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "658304fd40547ad30e221f1f6824feb4536224b38ad65e65a807dc0873c2e463"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python-data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
